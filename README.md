# Advent of Code with ChatGPT 2022

In this project I'll try to use [ChatGPT](https://chat.openai.com/chat) to solve the 2022 [Advent of Code](https://adventofcode.com/2022) puzzles.

The goal is to make ChatGPT come to the right solution without manually touching the code by guiding it to the right changes to make. I will however be using my knowledge as a programmer to sometimes give specific hints on how to solve any issues with his code. It's not a test to see if ChatGPT can solve these puzzles without any hints/human input, it's meant to see how adaptable it can be and how useful it would be as a pair program partner.

This way I can learn more about how ChatGPT operates, what the best methodolgy is to use it, how to finetune my prompts, and to get a scope of its capabilities.

## Structure

The project contains a folder for every challenge. Each folder contains subfolders for every attempt if it took more than one attempt with:

- A **conversation.md** file, which contains the conversation between me and ChatGPT.
- A **conversation_screenshot.png** file, which contains a screenshot of the conversation.
- Some amount of **attempt_*.py** files, which contain the code generated by ChatGPT in each iteration.
- An **input.txt** file, which contains the input for the challenge.
- A **conversation_summary_by_chatgpt.md** file, which contains a summary of the conversation written by ChatGPT as a final prompt.

## Results

### Day 1
 - *Attempt 1*: Was going well, but session expired and lost the conversation ðŸ¤¦â€â™‚ï¸.
 - *Attempt 2*: Solved in **3 attempts** but forgot about the second part of the puzzle ðŸ¤¦â€â™‚ï¸.
 - *Attempt 3*: Solved the first part in **3 attempts** and the second part in just **1 attempt**.

Notes:
- So impressive. Makes me wonder if the model is somehow training on the input from other people who also make it solve the same AoC challenges. Or there is some other kind of 'data leakage' for these specific challenges.

### Day 2
Solved first part in **3 attempts** and second part in **2 attempts**.

Notes:
- Most of the time for part 1 was spent by me debugging the almost perfect code. Should probably try to reproduce and try to debug via ChatGPT (but that's more overhead in the copy pasting of code).
- The biggest challenge of part 2 was keeping the code short enough so it didn't get cut off. Also, removing comments from the code is apparently not trivial for some reason.
- I had a lot more attempts for part 2 but none of the other ones were fully printed, so I only include those that are full pieces of code as attempt.

### Day 3
Solved both parts in **2 attempts**.

Notes:
- A very minor mistake in both parts, easily fixed by ChatGPT with simple instructions. Amazing.
- Output like this after being corrected makes it almost feel *too* human-like:
    > I mistakenly assumed that duplicates would be eliminated because the puzzle states that "the Elves have made a list of all of the items currently in each rucksack," which implies that each item is only counted once. However, upon rereading the puzzle description, I see that I was incorrect.

### Day 4
Part one took some tries, **5 attempts** to be exact, but part two made up for it by being solved in just **1 attempt**.

Notes:
- A new development in answering code. He gives the main part with empty functions but adds their full signatures and an explanation of what the implementation could look like. I like this approach, since it makes for less cutting off of code and makes working on specific parts more inuitive.
- Part 2 was solved in one attempt, which was great, because I actually misinterpreted the instructions and would have needed at least 2 tries myself without ChatGPT ðŸ™ƒ.

### Day 5
- *Attempt 1*: With our approach - making him aware of mistakes instead of actively giving hints on how to solve them - he's truggling a *lot* with the input parsing and it's simply taking too long. Although with enough time, I think we'd still get there. Let's try a more direct approach:
- *Attempt 2*: Give concrete hints on how to solve problems, while still not changing anything to the generated code. Part one was solved in **8 "attempts"** although attempts is probably not the correct metric to use anymore. The problem was solved step by step, function by function, so there wasn't any fully testable code until all steps were correctly resolved. Therefore, the amount of prompts might be a better metric: **10 prompts** in this case. Part two was solved in **3 attempts** or **6 prompts**, but we were thrown out by the session expiring, so we needed to provide the context from the lost conversation which might have increased the amount of attempts/prompts slightly.

Notes:
- Not a lot of explanation given in the first attempt, which was unusual.
- The logic for solving the puzzle was pretty okay. The reading of the input was the real challenge. He didn't take the examples as literal input, but rather assumed that the real input didn't have the brackets (or just wasn't smart enough to immediately take those into account.) He also assumed there were always going to be 3 stacks, which is not unreasonable since it's never explicitely stated.
- ChatGPT was also struggling with demand it seems. Lots of waiting for responses, trying again, etc.
- In conclusion: With the code of the very first attempt, I could have manually made the necessary changes pretty easily, but letting him find those needed changes by just pointing him towards them: not so easy. Providing him with concrete ways to solve the issues: much easier.

## Notes

- Sometimes, presumably when ChatGPT's answers are too long, he just stops. If this happens I need to urge him to shorten his answer. I do not count these prompts as attempts.

